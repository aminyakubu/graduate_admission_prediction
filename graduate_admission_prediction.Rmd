---
title: "Graduate Admission Prediction"
author: "Amin Yakubu"
date: "4/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

# Loading libraries
```{r}
library(tidyverse)
library(caret)
library(glmnet)
library(MASS)
library(e1071)
library(mlbench)
library(pROC)
library(AppliedPredictiveModeling)
library(gam)
library(car)
library(pdp)
library(earth)
library(rpart) 
library(rpart.plot)
library(party) 
library(partykit) 
library(randomForest) 
library(ranger) 
library(gbm) 
library(plotmo)
library(pdp) 
library(lime)
```

# Load Data & Clean

The variable `serial_no` is removed because it's the unique identifier 

```{r}
admit_df = read_csv('./data/Admission_Predict_Ver1.1.csv') %>% 
  janitor::clean_names() 

admit_df = admit_df[,-1]
```

Checking for missing data 

```{r}
sum(is.na(admit_df))
```

Here, we see that we have no missing data in the dataset

```{r}
X = model.matrix(chance_of_admit~., admit_df)[,-1]
y = admit_df$chance_of_admit
```

```{r}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
featurePlot(X, y, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(4, 2))
```

Each predictor is plotted against the response, `chance_of_admit`. The continuous variables `cgpa`, `gre_score`, and `toefl_score` all have a positive linear relationship with the outcome. This also appears to be the case with the categorical variables, with the response increasing on average with each increase in predictor value.

From the plots we can see that most of our variables are linearly distributed. Because of that we will start with simple linear regression

```{r}
corrplot::corrplot(cor(X))
```

The predictors in this dataset 

```{r}
summary(admit_df)
```

# Linear regression

Let's start with a linear regression

```{r}
seed = 2
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
```

# Linear Regression

```{r}
set.seed(seed)
lm.fit <- train(chance_of_admit ~., admit_df,
                method = "lm",
                trControl = ctrl1)

getTrainPerf(lm.fit)
```

# GAM

GRE score and toefl scores shows some none linear trends at the top

```{r}
set.seed(seed)
gam.fit <- train(chance_of_admit ~., admit_df,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE, FALSE)),
                 trControl = ctrl1)

gam.fit$bestTune
gam.fit$finalModel

gam.fit2 = gam(chance_of_admit ~ lor +
                s(cgpa) +
                research +
                s(gre_score) +
                s(toefl_score) +
                university_rating +
                sop,
                data = admit_df, method = 'GCV.Cp')

plot(gam.fit2)
gam.check(gam.fit2)
summary(gam.fit2)
```

## Multivariate Adaptive Regression Splines (MARS)

```{r}
mars_grid <- expand.grid(degree = 1:4, 
                         nprune = 2:15)
set.seed(seed)
mars.fit <- train(chance_of_admit ~., admit_df,
                 method = "earth",
                 tuneGrid = mars_grid,
                 trControl = ctrl1)
ggplot(mars.fit)
mars.fit$bestTune
coef(mars.fit$finalModel)
```

To better understand the relationship between these features and `chance_of_admit`, we can create partial dependence plots (PDPs) for each feature individually and also an interaction PDP. This is used to examine the marginal effects of predictors.

```{r}
p1 <- partial(mars.fit, pred.var = c("gre_score"), grid.resolution = 10) %>% autoplot()
p2 <- partial(mars.fit, pred.var = c("gre_score", "toefl_score"), grid.resolution = 10) %>% plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
            screen = list(z = 20, x = -60))

grid.arrange(p1, p2, ncol = 2)
```

# Regularized methods 

# Lasso

```{r}
set.seed(2)
lasso.fit <- train(chance_of_admit ~., admit_df,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 1, 
                                            lambda = exp(seq(-11,11, length = 300))),
                     trControl = ctrl1)

plot(lasso.fit, xTrans = function(x) log(x))

getTrainPerf(lasso.fit)
lasso.fit$bestTune$lambda
```

# Ridge

```{r}
set.seed(seed)
ridge.fit <- train(chance_of_admit ~., admit_df,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 0, 
                                            lambda = exp(seq(-11, 11, length = 200))),
                     trControl = ctrl1)

plot(ridge.fit, xTrans = function(x) log(x))

getTrainPerf(lasso.fit)
ridge.fit$bestTune$lambda
```

# Elastic Net

```{r}
set.seed(2)
enet.fit <- train(chance_of_admit ~., admit_df,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = seq(0, 1, length = 6), 
                                            lambda = exp(seq(-11, 11, length = 200))),
                     trControl = ctrl1)

plot(enet.fit)
enet.fit$bestTune
ggplot(enet.fit)
```

# PCR

```{r}
set.seed(seed)
pcr.fit <- train(chance_of_admit ~., admit_df,
                 method = "pcr",
                 tuneLength = 7,
                 trControl = ctrl1,
                 scale = T)

ggplot(pcr.fit, highlight = T)
```


# Tree-Based Models

```{r}
set.seed(seed)
# tune over cp, method = "rpart"
rpart.fit <- train(chance_of_admit ~., admit_df, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 20))),
                   trControl = ctrl1)

ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
```

# Bagging

```{r}
bagging.grid <- expand.grid(mtry = 7, 
                       splitrule = "variance",
                       min.node.size = 1:10) 
set.seed(seed)
bagging <- train(chance_of_admit ~., admit_df, 
                method = "ranger",
                tuneGrid = bagging.grid,
                trControl = ctrl1,
                importance = 'permutation')

ggplot(bagging, highlight = TRUE)

barplot(sort(ranger::importance(bagging$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

bagging$results[which.min(bagging$results[,5]),]
```

# Random forest

```{r}
rf.grid = expand.grid(mtry = 1:3, 
                       splitrule = "variance",
                       min.node.size = 1:10)
set.seed(seed)
rf.fit = train(chance_of_admit ~., admit_df, 
                method = "ranger", 
                tuneGrid = rf.grid,
                trControl = ctrl1,
                importance = 'permutation')
ggplot(rf.fit, highlight = TRUE)

barplot(sort(ranger::importance(rf.fit$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```


#GBM 

```{r}
gbm.grid = expand.grid(n.trees = c(1000, 2000,3000, 5000),
                        interaction.depth = 2:5, 
                        shrinkage = c(0.01,0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(seed)
gbm.fit = train(chance_of_admit ~., admit_df, 
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 verbose = TRUE,
                 trControl = ctrl1)
ggplot(gbm.fit, highlight = TRUE)
```

```{r}
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

Partial Dependence Plot 
```{r}
pdp.rf <- rf.fit %>% 
  partial(pred.var = "cgpa", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = admit_df) +
  ggtitle("Random forest") 

pdp.gbm <- gbm.fit %>% 
  partial(pred.var = "cgpa", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = admit_df) +
  ggtitle("Boosting") 

grid.arrange(pdp.rf, pdp.gbm, nrow = 1)
```

PDPs plot the change in the average predicted value as specified feature(s) vary over their marginal distribution. The PDP plot below displays the average change in predicted `chance_of_admit` as we vary `cgpa` while holding all other variables constant. 

# ICE 

```{r}
ice1.rf <- rf.fit %>% 
  partial(pred.var = "cgpa", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = admit_df, alpha = .1) +
  ggtitle("Random forest, non-centered") 

ice2.rf <- rf.fit %>% 
  partial(pred.var = "cgpa", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = admit_df, alpha = .1, 
           center = TRUE) +
  ggtitle("Random forest, centered") 

ice1.gbm <- gbm.fit %>% 
  partial(pred.var = "cgpa", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = admit_df, alpha = .1) +
  ggtitle("Boosting, non-centered") 

ice2.gbm <- gbm.fit %>% 
  partial(pred.var = "cgpa", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = admit_df, alpha = .1, 
           center = TRUE) +
  ggtitle("Boosting, centered") 

grid.arrange(ice1.rf, ice2.rf, ice1.gbm, ice2.gbm,
             nrow = 2, ncol = 2)
```

we are plotting the change in the predicted chance of admit for each observation as we vary each predictor variable. 

# Vanilla Neural Network - single layer neural network

```{r}
nnetGrid = expand.grid(size = seq(from = 5, to = 50, by = 5), 
                        decay = exp(seq(from = -6, to = -1, length = 15)))
set.seed(seed)
rnnet.fit = train(x = X, y = y,
                   method = "nnet",
                   tuneGrid = nnetGrid,
                   preProcess = c("center","scale"),
                   trControl = ctrl1,
                   linout = TRUE,
                   trace = FALSE)

ggplot(rnnet.fit, highlight = TRUE) + scale_shape_manual(values = rep(19,10), guide = FALSE)
```

Multi - hidden layer neural network using multilayer perceptron (backpropagation)

```{r}
mlp_grid = expand.grid(layer1 = 10,
                       layer2 = 10,
                       layer3 = 10)

mlp_fit = train(x = X, 
                y = y, 
                method = "mlpML", 
                preProc =  c('center', 'scale'),
                trControl = ctrl1,
                tuneGrid = mlp_grid)

```

```{r}
resamp <- resamples(list(lm = lm.fit, 
                         gam = gam.fit,
                         mars = mars.fit,
                         ridge = ridge.fit, 
                         lasso = lasso.fit,
                         enet = enet.fit,
                         pcr = pcr.fit,
                         decision_tree = rpart.fit,
                         bagging = bagging,
                         random_forest = rf.fit,
                         boosting = gbm.fit,
                         vanilla_NN = rnnet.fit,
                         multi_NN = mlp_fit))
summary(resamp)
```

```{r}
bwplot(resamp, metric = "RMSE")
```

