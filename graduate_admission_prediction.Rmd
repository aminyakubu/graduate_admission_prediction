---
title: "Graduate Admission Prediction"
author: "Amin Yakubu"
date: "4/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

# Loading libraries
```{r}
library(tidyverse)
library(caret)
library(glmnet)
library(MASS)
library(e1071)
library(mlbench)
library(pROC)
library(AppliedPredictiveModeling)
library(gam)
library(car)
```

# Load Data

```{r}
admit_df = read_csv('./data/Admission_Predict_Ver1.1.csv')
```

Checking for missing data 
```{r}
sum(is.na(admit_df))
```

Here, we see that we have no missing data in the dataset

Removal of unnecessary variables
```{r}
admit_df = admit_df %>% janitor::clean_names() 
admit_df = admit_df[,-1]
```

The variable `serial_no` is removed.

```{r}
X = model.matrix(chance_of_admit~., admit_df)[,-1]
y = admit_df$chance_of_admit
```

```{r}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
featurePlot(X, y, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(4, 2))
```

Each predictor is plotted against the response, `chance_of_admit`. The continuous variables `cgpa`, `gre_score`, and `toefl_score` all have a positive linear relationship with the outcome. This also appears to be the case with the categorical variables, with the response increasing on average with each increase in predictor value.

From the plots we can see that most of our variables are linearly distributed. Because of that we will start with simple linear regression

```{r}
corrplot::corrplot(cor(X))
```

The predictors in this dataset 

```{r}
summary(admit_df)
```

Let's start with a simple linear regression

```{r}
fit = lm(chance_of_admit~., admit_df)
summary(fit)
```

```{r}
vif(fit)
```

We see that `university_rating` and `sop` are not associated with chance of admission due to their large p values. However, we will not remove these variables as we believe they are significant contributors to whether or not a student is admitted. 

We also see that the no variance inflation factors are above 10, indicating that there is no multicollinearity.

Diagnositcs 

```{r}
par(mfrow = c(2,2)) 
plot(fit)
```

```{r}
#this is code used to remove "university_rating" and "sop"
#admit_df = admit_df[,-c(3, 4)]
```

```{r}
X = model.matrix(chance_of_admit~., admit_df)[,-1]
y = admit_df$chance_of_admit
```

```{r}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

```

# Linear Regression

```{r}
set.seed(2)
lm.fit <- train(X, y,
                method = "lm",
                trControl = ctrl1)

getTrainPerf(lm.fit)

```

# GAM

GRE score and toefl scores shows some none linear trends at the top

```{r}
set.seed(2)
gam.fit <- train(X, y,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE, FALSE)),
                 trControl = ctrl1)

gam.fit$bestTune
gam.fit$finalModel
```

## Multivariate Adaptive Regression Splines (MARS)

```{r}
library(pdp)
library(earth)
```

```{r}
mars_grid <- expand.grid(degree = 1:4, 
                         nprune = 2:15)
set.seed(2)
mars.fit <- train(X, y,
                 method = "earth",
                 tuneGrid = mars_grid,
                 trControl = ctrl1)
ggplot(mars.fit)
mars.fit$bestTune
coef(mars.fit$finalModel)
```

To better understand the relationship between these features and `chance_of_admit`, we can create partial dependence plots (PDPs) for each feature individually and also an interaction PDP. This is used to examine the marginal effects of predictors.

```{r}
p1 <- partial(mars.fit, pred.var = c("gre_score"), grid.resolution = 10) %>% autoplot()
p2 <- partial(mars.fit, pred.var = c("gre_score", "toefl_score"), grid.resolution = 10) %>% plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
            screen = list(z = 20, x = -60))

grid.arrange(p1, p2, ncol = 2)
```

# Regularized methods 

# Lasso

```{r}
set.seed(2)
lasso.fit <- train(X, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 1, 
                                            lambda = exp(seq(-11,11, length = 300))),
                   # preProc = c("center", "scale"),
                     trControl = ctrl1)

plot(lasso.fit, xTrans = function(x) log(x))

getTrainPerf(lasso.fit)
lasso.fit$bestTune$lambda
```

# Ridge

```{r}
set.seed(2)
ridge.fit <- train(X, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 0, 
                                            lambda = exp(seq(-11, 11, length = 300))),
                     trControl = ctrl1)

plot(ridge.fit, xTrans = function(x) log(x))

getTrainPerf(lasso.fit)
ridge.fit$bestTune$lambda
```

# Elastic Net

```{r}
set.seed(2)
enet.fit <- train(X, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = seq(0, 1, length = 5), 
                                            lambda = exp(seq(-2, 4, length = 50))),
                     trControl = ctrl1)

plot(enet.fit)
enet.fit$bestTune
ggplot(enet.fit)
```

# PCR

```{r}
set.seed(2)
pcr.fit <- train(X, y,
                 method = "pcr",
                 tuneLength = 7,
                 trControl = ctrl1,
                 scale = T)

plot(pcr.fit)

pcr.pred <- predict(pcr.fit$finalModel, newdata = X, 
                    ncomp = pcr.fit$bestTune$ncomp)

mean((pcr.pred - y)^2)
```

```{r}
resamp <- resamples(list(lasso = lasso.fit, 
                         ridge = ridge.fit, 
                         pcr = pcr.fit,
                         lm = lm.fit,
                         enet = enet.fit,
                         mars = mars.fit,
                         gam = gam.fit))
summary(resamp)
```

```{r}
bwplot(resamp, metric = "RMSE")
```
