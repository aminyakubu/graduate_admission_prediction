---
title: "Graduate Admission Prediction"
author: "Amin Yakubu"
date: "4/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

# Loading libraries
```{r}
library(tidyverse)
library(caret)
library(glmnet)
library(MASS)
library(e1071)
library(mlbench)
library(pROC)
library(AppliedPredictiveModeling)
```

# Load Data

```{r}
admit_df = read_csv('data/Admission_Predict_Ver1.1.csv')
```

#Pre-processing Data

Checking for missing data 
```{r}
sum(is.na(admit_df))
```

Here, we see that we have no missing data in the dataset

Removal of unnecessary variables
```{r}
admit_df = admit_df %>% janitor::clean_names() 
admit_df = admit_df[,-1]
```

The variable `serial_no` is removed.

```{r}
summary(lm(chance_of_admit~., admit_df))
```

We will remove the variables `university_rating` and `sop`, 

```{r}
admit_df = admit_df[,-c(3, 4)]
```


Find Linear Dependencies
```{r}
findLinearCombos(admit_df)
```

There are no linear dependencies and therefore no predictors need to be removed.

```{r}
X = model.matrix(chance_of_admit~., admit_df)[,-1]
y = admit_df$chance_of_admit
```

```{r}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
featurePlot(X, y, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(4, 2))
```

Each predictor is plotted against the response, `chance_of_admit`. The continuous variables `cgpa`, `gre_score`, and `toefl_score` all have a positive linear relationship with the outcome. This also appears to be the case with the categorical variables, with the response increasing on average with each increase in predictor value.

From the plots we can see that most of our variables are linearly distributed. Because of that we will start with simple linear regression

```{r}
corrplot::corrplot(cor(X))
```


```{r}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
```


# Linear Regression

```{r}
set.seed(2)
lm.fit <- train(X, y,
                method = "lm",
                trControl = ctrl1)

lm.pred <- predict(lm.fit$finalModel, newdata = data.frame(X))
mean((lm.pred - y)^2)
```

# Lasso

```{r}
set.seed(2)
lasso.fit <- train(X, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 1, 
                                            lambda = exp(seq(-10,10, length = 200))),
                   # preProc = c("center", "scale"),
                     trControl = ctrl1)

plot(lasso.fit, xTrans = function(x) log(x))

bestlam.lasso = lasso.fit$bestTune$lambda

lasso.pred = predict(lasso.fit$finalModel, s = bestlam.lasso, newx = X)
mean((lasso.pred - y)^2)
```

# Ridge

```{r}
set.seed(2)
ridge.fit <- train(X, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 0, 
                                            lambda = exp(seq(-10, 10, length = 200))),
                     trControl = ctrl1)

plot(ridge.fit, xTrans = function(x) log(x))

bestlam.ridge = ridge.fit$bestTune$lambda

ridge.pred = predict(ridge.fit$finalModel, s = bestlam.ridge, newx = X)
mean((ridge.pred - y)^2)
```

# PCR

```{r}
set.seed(2)
pcr.fit <- train(X, y,
                 method = "pcr",
                 tuneLength = 7,
                 trControl = ctrl1,
                 scale = T)

plot(pcr.fit)

pcr.pred <- predict(pcr.fit$finalModel, newdata = X, 
                    ncomp = pcr.fit$bestTune$ncomp)
mean((pcr.pred - y)^2)
```

# Elastic Net

```{r}
set.seed(2)
enet.fit <- train(X, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = seq(0, 1, length = 5), 
                                            # We are seeing if alpha between 0 and 1 is better
                                            lambda = exp(seq(-2, 4, length = 50))),
                   # preProc = c("center", "scale"),
                     trControl = ctrl1)

# for each alpha we have 50 lambdas. 
enet.fit$bestTune

ggplot(enet.fit)
```

# GAM

```{r}
set.seed(2)
gam.fit <- train(X, y,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE,FALSE)),
                 trControl = ctrl1)

gam.fit$bestTune
gam.fit$finalModel
```

## Multivariate Adaptive Regression Splines (MARS)

```{r}
library(pdp)
library(earth)
```

```{r}
mars_grid <- expand.grid(degree = 1:2, 
                         nprune = 2:10)
set.seed(2)
mars.fit <- train(X, y,
                 method = "earth",
                 tuneGrid = mars_grid,
                 trControl = ctrl1)
ggplot(mars.fit)
mars.fit$bestTune
coef(mars.fit$finalModel)
```

To better understand the relationship between these features and `lpsa`, we can create partial dependence plots (PDPs) for each feature individually and also an interaction PDP. This is used to examine the marginal effects of predictors.

```{r}
p1 <- partial(mars.fit, pred.var = c("gre_score"), grid.resolution = 10) %>% autoplot()
p2 <- partial(mars.fit, pred.var = c("gre_score", "toefl_score"), grid.resolution = 10) %>% plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
            screen = list(z = 20, x = -60))

grid.arrange(p1, p2, ncol = 2)
```

```{r}
resamp <- resamples(list(lasso = lasso.fit, 
                         ridge = ridge.fit, 
                         pcr = pcr.fit,
                         lm = lm.fit,
                         enet = enet.fit,
                         mars = mars.fit,
                         gam = gam.fit))
summary(resamp)
```

```{r}
bwplot(resamp, metric = "RMSE")
```
